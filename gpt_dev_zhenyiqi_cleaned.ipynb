{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcsOz1YaneSKRaRZmuN3s6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenyiqi/nanoGPT/blob/master/gpt_dev_zhenyiqi_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "W6DX4Vkz-aBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ImyW8FZR-bJW"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "jHkLhsd1-cqE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset"
      ],
      "metadata": {
        "id": "pg1vzWw2EqQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "JZQEqOCMEsiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvw-FhHyvCSd",
        "outputId": "718922cb-9d5d-47b5-a2d3-a76f00d0ac5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-12 18:56:15--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-04-12 18:56:15 (19.2 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "of4E6yTIzoTi"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of the dataset is {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7roPlm1kzxvC",
        "outputId": "b07b6e70-76f2-42b7-ef9d-1ef141fff371"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset is 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encode characters into numbers (indices)"
      ],
      "metadata": {
        "id": "Mt-3XWCCEwiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "1z9uimoSz4KS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vbt-EjT0DW3",
        "outputId": "04edaca2-737d-4227-b425-98c199c07bad"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "7uY5sj9V0J0H"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda i: ''.join([itos[ind] for ind in i])"
      ],
      "metadata": {
        "id": "o1C6NGVKN-fw"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello there\"))\n",
        "print(decode(encode(\"hello there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQNJTU2GOdbX",
        "outputId": "d3d27a24-b3df-438a-ac7a-8ceb62673963"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
            "hello there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "JQi1cx8gOhTA"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQByFZ_0OlZD",
        "outputId": "e5fefaaa-8036-4f0f-fea0-ba6deecaf510"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train/validation sets"
      ],
      "metadata": {
        "id": "gCnS42TqFB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "uULTmW4iPnCu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notation\n",
        "block_size = 8 # this is the maximum length of the chunk (sequence length, in other places, this is also donated as T)\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX1I9zWUP6Qx",
        "outputId": "c295ed73-7c65-4b9b-ba3d-1fe3224df2d0"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # [18, 47, 56, 57, 58,  1, 15, 47]\n",
        "y = train_data[1:block_size + 1] # [47, 56, 57, 58,  1, 15, 47, 58]\n",
        "for t in range(block_size):\n",
        "  context = x[:t + 1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context}, the target is : {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0rf_M4LQS0R",
        "outputId": "323cb438-8c72-4813-fc76-1a90e528d632"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]), the target is : 47\n",
            "when input is tensor([18, 47]), the target is : 56\n",
            "when input is tensor([18, 47, 56]), the target is : 57\n",
            "when input is tensor([18, 47, 56, 57]), the target is : 58\n",
            "when input is tensor([18, 47, 56, 57, 58]), the target is : 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]), the target is : 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is : 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is : 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generalize with extra batch size dimension\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # a random vector of length batch_size, with its value sampled from\n",
        "  # the range (0, len(data) - block_size), which is all possible values for the\n",
        "  # the start of a sampled sequence.\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "  # now get the samples out, stacking them together\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])  # (block_size, batch_size)\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (block_size, batch_size)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "O9CWiLogRJ7R"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "jpJC7VNxTlR4"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEQlFjtNTnnw",
        "outputId": "af7d052e-e2f0-46f9-da16-643832dc5ad5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()}, the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzjutp5qTwuI",
        "outputId": "d2017466-e88b-47b1-8ad0-43ad0675168b"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [24], the target is 43\n",
            "when input is [24, 43], the target is 58\n",
            "when input is [24, 43, 58], the target is 5\n",
            "when input is [24, 43, 58, 5], the target is 57\n",
            "when input is [24, 43, 58, 5, 57], the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1], the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
            "when input is [44], the target is 53\n",
            "when input is [44, 53], the target is 56\n",
            "when input is [44, 53, 56], the target is 1\n",
            "when input is [44, 53, 56, 1], the target is 58\n",
            "when input is [44, 53, 56, 1, 58], the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46], the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
            "when input is [52], the target is 58\n",
            "when input is [52, 58], the target is 1\n",
            "when input is [52, 58, 1], the target is 58\n",
            "when input is [52, 58, 1, 58], the target is 46\n",
            "when input is [52, 58, 1, 58, 46], the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39], the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
            "when input is [25], the target is 17\n",
            "when input is [25, 17], the target is 27\n",
            "when input is [25, 17, 27], the target is 10\n",
            "when input is [25, 17, 27, 10], the target is 0\n",
            "when input is [25, 17, 27, 10, 0], the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21], the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpliXu4AWnfc",
        "outputId": "946ef46b-4277-42d8-83e9-b0881e617aba"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "-jWpcHn0FK2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "Sa8hXq2X10AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "wUmjXHoh2rpf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LayerNorm(nn.Module):\n",
        "#   def __init__(self, dim, eps=1e-5):\n",
        "#     self.eps = eps\n",
        "      # gamma and beta are trainable variables\n",
        "#     self.gamma = torch.ones(dim)\n",
        "#     self.beta = torch.ones(dim)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     xmean = x.mean(1, keepdim=True)\n",
        "#     xvar = x.var(1, keepdim=True)\n",
        "#     xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "#     out = self.gamma * xhat + self.beta\n",
        "#     return out\n",
        "\n",
        "#   def parameters(self):\n",
        "#     return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "e8zZsSkh10AH"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self-attention head with dropout"
      ],
      "metadata": {
        "id": "0rKspkVb3A8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "causal self-attention.\n",
        "\n",
        "1. Originally Q and K are of dimension (T, head_size), ignoring the batch dimension, where `T` is the (max) sequence length (or block size).\n",
        "\n",
        "1. The overlap between the Q and K is calculate by taking inner product of the two matrix, resulting in a matrix of shape (T, T)\n",
        "```\n",
        "     [q1k1  q1k2, q1k3, q1k4, ..., q1kt]\n",
        "     [q2k1, q2k2, q2k3, q2k4, ..., q2kt]\n",
        "     [q3k1, q3k2, q3k3, q3k4, ..., q3kt]\n",
        "     [...                          ....]\n",
        "     [qtk1, qtk2, ...              qtkt]\n",
        "```\n",
        "\n",
        "1. The matrix is masked such that the all $q_i k_j$, where $j > i$ terms should be ignored. We first make them `-∞`.\n",
        "```\n",
        "     [q1k1  -inf, -inf, -inf, ..., -inf]\n",
        "     [q2k1, q2k2, -inf, -inf, ..., -inf]\n",
        "     [q3k1, q3k2, q3k3, -inf, ..., -inf]\n",
        "     [...                          ....]\n",
        "     [qtk1, qtk2, ...              qtkt]\n",
        "```\n",
        "After softmax, these `-∞` terms will become zero\n",
        "1. Then we calculate the weighted value by multiplying this weight (Q * K) with V, which is of dimension (T, head_size).\n",
        "\n",
        "* input shape: (T, n_embed)\n",
        "* output shape: (T, head_size)\n",
        "* what's the input: a sequence of tokens, that acts as Q, K and V; T is sequence length and n_embed is the embedded token."
      ],
      "metadata": {
        "id": "_VYhx_WBR00U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    \"\"\"self-attention head.\n",
        "\n",
        "    there will be multiple heads. head_size * n_head = n_embed.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    # tril is not the parameter of the module, so we call it a buffer\n",
        "    # (not a parameter)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape # C is the head_size here\n",
        "    k = self.key(x) # (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, head_size)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * C ** -0.5 # (B, T, T)\n",
        "    # masking terms like q_i * k_j, where j > i.\n",
        "    # The masked matrix looks like\n",
        "\n",
        "    # [q1k1  -inf, -inf, -inf, ..., -inf]\n",
        "    # [q2k1, q2k2, -inf, -inf, ..., -inf]\n",
        "    # [q3k1, q3k2, q3k3, -inf, ..., -inf]\n",
        "    # [...                          -inf]\n",
        "    # [qtk1, qtk2, ...              qtkt]\n",
        "\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "    # The softmax will get normalize all the non-zero terms and make all the\n",
        "    # -inf terms zero.\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x) # (B, T, head_size)\n",
        "    out = wei @ v # (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "ovuFWAYe3Acc"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: How many training parameters here?**\n",
        "\n",
        "The `nn.Linear` will have n_embed * head_size. There are three of them, so we have\n",
        "\n",
        "**n_embed * head_size * 3** training parameters"
      ],
      "metadata": {
        "id": "RtwSTbL6UpB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention with a projection"
      ],
      "metadata": {
        "id": "AGzrj5Ms10AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that after the SelfAttention head, we have output of shape (T, head_size), where `T` is the (max) sequence length and head_size * num_head = n_embed.\n",
        "\n",
        "We need to repeat the same thing num_head times. Of course we can trivially do this, resulting in num_head of (T, head_size) matrix. But these matrix will need to be concatenated so that we're back at (T, head_size * num_head) matrix.\n",
        "\n",
        "Then we do a projection from n_embed to n_embed.\n",
        "\n",
        "The concatenation is done by `torch.cat()`.\n",
        "\n",
        "* Input shape: (T, head_size)\n",
        "* Output shape: (T, n_embed)"
      ],
      "metadata": {
        "id": "I9RBlU-GTlsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(attention_out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "o-Etq2Uc10AI"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: how many training parameters here?\n",
        "\n",
        "each SelfAttentionHead has n_embed * head_size * 3 parameters, and we have num_head of heads, so the total number of parameters for heads are n_embed * head_size * num_head * 3 = 3 * n_embed^2, because head_size * num_head == n_embed.\n",
        "\n",
        "There is another projection layer that contributes to n_embed * n_embed number of paramters. So the total number of parameters in MultiHeadAttention (which includes selfAttentionHead) is\n",
        "\n",
        "**4 * n_embed ^ 2**"
      ],
      "metadata": {
        "id": "OX8SJ302U9Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FeedForward with dropout"
      ],
      "metadata": {
        "id": "vuVOS4Zt10AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we do non-fancy stuffs. Feedforward neural network. Within the network, we make things of higher dimension, so the neural network looks like\n",
        "```\n",
        "    ||\n",
        "    ||\n",
        "    ||\n",
        "  ||||||\n",
        "in||||||out\n",
        "  ||||||\n",
        "    ||\n",
        "    ||\n",
        "    ||\n",
        "```\n",
        "\n",
        "* input shape: (T, n_embed)\n",
        "* output shape: (T, n_embed)"
      ],
      "metadata": {
        "id": "RTOjY1ADV0Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed * 4, bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_embed * 4, n_embed, bias=False),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "UZYDYjV610AI"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q: how many training parameters here?**\n",
        "\n",
        "Linear will have (n_embed * n_embed * 4) parameters\n",
        "\n",
        "ReLu have 0 training parameters.\n",
        "\n",
        "The second Linear will have (4 * n_embed * n_embed) parameters\n",
        "\n",
        "So in total, there are\n",
        "\n",
        "**8 * e_embed^2** parameters in the feedforward component"
      ],
      "metadata": {
        "id": "g_5F5TUEWEIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block of (self-attention and feed-forward)"
      ],
      "metadata": {
        "id": "6_H-cvaw10AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's basically just (self-attention and feed-forward but we also add layer norms and skip add)\n",
        "\n",
        "The skip add is applied to each MultiHeadAttention layer and the feedforward layer.\n",
        "\n",
        "* input shape: (T, n_embed)\n",
        "* output shape: (T, n_embed)"
      ],
      "metadata": {
        "id": "Q270RhEmYk4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size=head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed, bias=False)\n",
        "    self.ln2 = nn.LayerNorm(n_embed, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "8GCmWoNJ10AJ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many training parameters**.\n",
        "\n",
        "We add the training parameters of the multi-headed self attention with the feedforward NN, it would be our answer:\n",
        "\n",
        "12 * n_embed^2\n",
        "\n",
        "Additionally, we have two LayerNorm layers, each giving n_embed training parameters (bias = false)\n",
        "\n",
        "2 * n_embed.\n",
        "\n",
        "In total, it'd be\n",
        "\n",
        "**2*n_embed + 12 * n_embed^2**"
      ],
      "metadata": {
        "id": "sLTtCXfxYwV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model with multiple layers of (self-attention, feed-forward) and layernorm + skipadd"
      ],
      "metadata": {
        "id": "jybCCY6J10AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put everything together and repeat the decoder module n_layer times with skipAdd layers\n",
        "\n",
        "* input shape: (T)\n",
        "* output shape: (T, vocab_size) + a scaler (the loss)"
      ],
      "metadata": {
        "id": "G2o18qaFZih3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embed = 32\n",
        "n_head = 4\n",
        "n_layer = 3\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # an embedding is a table that maps each token to a vector\n",
        "    # nn.Embedding(n1, n2) maps a token whose value is up to n1 (starting from 0)\n",
        "    # to a vector of size n2. Here we map each token to a vector of size vocab_size\n",
        "    # (One example is one-hot encoding)\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed, bias=False) # final layer norm\n",
        "    # This is the output layer that maps the transformed embeddings back to the\n",
        "    # original vocab_size vector - logits.\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"A Forward proporgation.\n",
        "\n",
        "    idx: the input. Dimension [B, T].\n",
        "    targets: target. Dimension [B, T], [B, i]th element is the target of sequence\n",
        "      idx[B, :i].\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B, T), meaning (batch_size, block_size)\n",
        "    token_embed = self.token_embedding_table(idx) # (B, T, C = n_embed)\n",
        "    # pos_embed = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    pos_embed = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
        "\n",
        "    x = token_embed + pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x) # (B, T, C = vocab_size)\n",
        "    # here is a bit tricky, please refer to\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "    # to understand what's in there\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      # reshapes the logits to a 2D tensor where each row represents a single\n",
        "      # data sample and each column represents a class\n",
        "      logits = logits.view(B * T, C)\n",
        "      # reshapes the targets to a 1D tensor where each element represents the\n",
        "      # true class label for a single data sample\n",
        "      targets = targets.view(B * T)\n",
        "\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"Generate the next token for max_new_tokens of times for the batch idx.\n",
        "\n",
        "    idx: a batch of samples, of size (B, T).\n",
        "    max_new_tokens: int, how many tokens to be generated given a sample.\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop the input size so that it's at most the same as block_size when\n",
        "      # generating\n",
        "      # for all the batches (idx[:, ...]), we choose the last block_size number\n",
        "      # of tokens; dimension (B, T)\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond) # logits: (B, T, C)\n",
        "      # Last one of the sequence\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
        "    return idx\n",
        "\n",
        "model = GPT()\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635c9f8a-d303-4879-b48f-8879627cc44a",
        "id": "QdxWeyaA10AJ"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 65])\n",
            "tensor(4.2418, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What's the number of training parameters?**\n",
        "\n",
        "* nn.Embedding for the tokens themselves has a dimension of (vocab_size, n_embed) so it contains (vocab_size * n_embed) parameters\n",
        "* nn.Embedding for the positions contains (T * n_embed)\n",
        "* now we have n_layer layers of decoder, each decoder, as we found earlier, has (12 * n_embed^2 + 2 * n_embed) number of parameters so in total we have 12 * n_layer * n_embed^2 + 2 * n_layer * n_embed parameters\n",
        "* we also add another LayerNorm layer at the end, contributing to n_embed number of parameters.\n",
        "* lastly, we have a linear head that maps the vector of length n_embed to vector of length vocab_size, this will give another (vocab_size * n_embed) parameters. adding them up, we got\n",
        "\n",
        "**(2 vocab_size + T + 2 n_layer + 1)  n_embed + 12 n_layer n_embed ^ 2** number of parameters\n",
        "\n",
        "For this one.\n",
        "n_embed = 32, n_layer = 3, vocab_size = 65\n",
        "T = 8 (determined dynamically during training and reused during inference)\n",
        "\n",
        "145 * 32 + 36 * 32 ^ 2 = 41504"
      ],
      "metadata": {
        "id": "P0ArD12ZZ0Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6CLZ5TD10AJ",
        "outputId": "011b27bc-9de3-4dad-8057-788235dbd13f"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41504"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "hK9UEXSWAQtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take model parameters as the input. This means that an optimizer is the object\n",
        "# that manages and iterates the model weights.\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jmWkQOi1A-un"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants"
      ],
      "metadata": {
        "id": "9dDuKl9stnYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "LV-YUNBfPEB2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For evaluation only\n",
        "# To tell Pytorch that everything inside this function will not be used in\n",
        "# loss.backward() so that it won't allocate any memory for the local variables\n",
        "# for backproporgation.\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "1j2eUN0RPmy0"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for iter in range(max_iters):\n",
        "  # estimate the loss every 500 iterations\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  # zero out the gradients before the training\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xatxcLsGBFUV",
        "outputId": "09a4cd74-a733-477a-f0e8-287d2766f2bd"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3104, val loss 4.3100\n",
            "step 500: train loss 2.4657, val loss 2.4560\n",
            "step 1000: train loss 2.3115, val loss 2.3273\n",
            "step 1500: train loss 2.2429, val loss 2.2598\n",
            "step 2000: train loss 2.1853, val loss 2.2123\n",
            "step 2500: train loss 2.1563, val loss 2.1893\n",
            "step 3000: train loss 2.1411, val loss 2.1585\n",
            "step 3500: train loss 2.1024, val loss 2.1458\n",
            "step 4000: train loss 2.0894, val loss 2.1271\n",
            "step 4500: train loss 2.0581, val loss 2.0932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "j7HmiyGgBwZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "print(decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jyE9E4tCC5o",
        "outputId": "9a9009fa-9139-43d4-f606-54a8092a3630"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MEONTUCES:\n",
            "Leash for hous?\n",
            "Tais magelianss that, tot shee perecaint:\n",
            "A nestiones'd me; but etsed a thoGough\n",
            "Cetrane:\n",
            "Aod theag\n",
            "Furs'd sownerreawirs that she aland letgieg,\n",
            "And yough, guk thatance opinair! vive; the loot.\n",
            "\n",
            "I wethyit? pare,\n",
            "Thirphe haty of not oun, cony the him wo af that's of mitte t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the checkpoint"
      ],
      "metadata": {
        "id": "0CYBBTJUCDSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "s4RsNgJNCX08",
        "outputId": "2848cd5f-e4c5-4dcb-a2f7-5f8c1e22103d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.txt  input.txt.1\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./checkpoints\""
      ],
      "metadata": {
        "id": "hJurWs8UCaVi"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'epoch': 5000,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}"
      ],
      "metadata": {
        "id": "eMJz5dVpDENK"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filename = 'model_checkpoint.pt'\n",
        "checkpoint_path = 'checkpoints/'"
      ],
      "metadata": {
        "id": "ZVuRWl7E1EEr"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "IUGQZ9x21QZa"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(checkpoint, os.path.join(checkpoint_path, checkpoint_filename))"
      ],
      "metadata": {
        "id": "Y764tzMU1LdR"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load from a checkpoint"
      ],
      "metadata": {
        "id": "eMh4a51x1P2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "another_model = GPT()\n"
      ],
      "metadata": {
        "id": "3Akd8mzF1bwB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = 'checkpoints/model_checkpoint.pt'"
      ],
      "metadata": {
        "id": "ekvq8Z3h1yAA"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(checkpoint_path)"
      ],
      "metadata": {
        "id": "Ds8qqoVu13Wi"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "another_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "another_optimizer = torch.optim.AdamW(another_model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "dG0-M7Bw1rwN"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "another_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "uSPBA47D2DpH"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "print(decode(another_model.generate(idx, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "id": "EEMs4I6u2Msg",
        "outputId": "f3bfb566-c157-4a0d-e094-2ee5c57157ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CRICPARoEl:'sels treatson\n",
            "Thany Ourth's a frind allor foull feralt'seforthe hath stink: bagiyily.\n",
            "\n",
            "KINGWCARBYEENAPER:\n",
            "LUEINGTABENRGCASIR:\n",
            "Buth-\n",
            "Whancch thou wenthed a gringinss of way you not that thene the hould; thut on posentay Kords\n",
            "Our lines; in not\n",
            "andyn I?\n",
            "Yevince with sold,\n",
            "So shairse, in th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload my model to HuggingFace (the code doesn't work yet)\n"
      ],
      "metadata": {
        "id": "M_tIM33k2RGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi"
      ],
      "metadata": {
        "id": "y7rSqEGK2uKh"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HuggingFaceToken')"
      ],
      "metadata": {
        "id": "F8TbOzUk2w3j"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi(token=hf_token)"
      ],
      "metadata": {
        "id": "72oEOykI3Cfg"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mini-gpt\""
      ],
      "metadata": {
        "id": "oY9omEFO3Xkn"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9re_EFn3eCh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}