{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNgAf3up0KM275rv4FbaI/T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhenyiqi/nanoGPT/blob/master/gpt_dev_zhenyiqi_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "W6DX4Vkz-aBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ImyW8FZR-bJW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "jHkLhsd1-cqE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare dataset"
      ],
      "metadata": {
        "id": "pg1vzWw2EqQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download"
      ],
      "metadata": {
        "id": "JZQEqOCMEsiw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvw-FhHyvCSd",
        "outputId": "7532382b-a484-4411-f31f-88ca1a80d5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-31 00:11:40--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-31 00:11:40 (20.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "of4E6yTIzoTi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of the dataset is {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7roPlm1kzxvC",
        "outputId": "701b6b92-9ee4-4bb8-abf7-eb6f5464f591"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset is 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## encode characters into numbers (indices)"
      ],
      "metadata": {
        "id": "Mt-3XWCCEwiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "1z9uimoSz4KS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vbt-EjT0DW3",
        "outputId": "452bedcb-e76e-4fae-a659-ec70f35c7ad1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "7uY5sj9V0J0H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda i: ''.join([itos[ind] for ind in i])"
      ],
      "metadata": {
        "id": "o1C6NGVKN-fw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hello there\"))\n",
        "print(decode(encode(\"hello there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQNJTU2GOdbX",
        "outputId": "58948f07-6d62-4e69-f038-7e81d90395b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
            "hello there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "JQi1cx8gOhTA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQByFZ_0OlZD",
        "outputId": "44b4176c-22fb-43ba-94cc-5d444747e09b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train/validation sets"
      ],
      "metadata": {
        "id": "gCnS42TqFB6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "uULTmW4iPnCu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notation\n",
        "block_size = 8 # this is the maximum length of the chunk (sequence length, in other places, this is also donated as T)\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX1I9zWUP6Qx",
        "outputId": "7aca4b38-fc9c-435f-92c4-a6dc95bdac83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size] # [18, 47, 56, 57, 58,  1, 15, 47]\n",
        "y = train_data[1:block_size + 1] # [47, 56, 57, 58,  1, 15, 47, 58]\n",
        "for t in range(block_size):\n",
        "  context = x[:t + 1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context}, the target is : {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0rf_M4LQS0R",
        "outputId": "36f35771-0a29-49f0-a777-06cf0dffaa1e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]), the target is : 47\n",
            "when input is tensor([18, 47]), the target is : 56\n",
            "when input is tensor([18, 47, 56]), the target is : 57\n",
            "when input is tensor([18, 47, 56, 57]), the target is : 58\n",
            "when input is tensor([18, 47, 56, 57, 58]), the target is : 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]), the target is : 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is : 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is : 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generalize with extra batch size dimension\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  # a random vector of length batch_size, with its value sampled from\n",
        "  # the range (0, len(data) - block_size), which is all possible values for the\n",
        "  # the start of a sampled sequence.\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "  # now get the samples out, stacking them together\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])  # (block_size, batch_size)\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # (block_size, batch_size)\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "O9CWiLogRJ7R"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')"
      ],
      "metadata": {
        "id": "jpJC7VNxTlR4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "print('----')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEQlFjtNTnnw",
        "outputId": "f771fc61-e9bd-4922-b599-8b8b4f712cdf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b, t]\n",
        "    print(f\"when input is {context.tolist()}, the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzjutp5qTwuI",
        "outputId": "61864ac8-5615-4b90-b2a2-be7b2ade543a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [24], the target is 43\n",
            "when input is [24, 43], the target is 58\n",
            "when input is [24, 43, 58], the target is 5\n",
            "when input is [24, 43, 58, 5], the target is 57\n",
            "when input is [24, 43, 58, 5, 57], the target is 1\n",
            "when input is [24, 43, 58, 5, 57, 1], the target is 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46], the target is 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39\n",
            "when input is [44], the target is 53\n",
            "when input is [44, 53], the target is 56\n",
            "when input is [44, 53, 56], the target is 1\n",
            "when input is [44, 53, 56, 1], the target is 58\n",
            "when input is [44, 53, 56, 1, 58], the target is 46\n",
            "when input is [44, 53, 56, 1, 58, 46], the target is 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39], the target is 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1\n",
            "when input is [52], the target is 58\n",
            "when input is [52, 58], the target is 1\n",
            "when input is [52, 58, 1], the target is 58\n",
            "when input is [52, 58, 1, 58], the target is 46\n",
            "when input is [52, 58, 1, 58, 46], the target is 39\n",
            "when input is [52, 58, 1, 58, 46, 39], the target is 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58], the target is 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46\n",
            "when input is [25], the target is 17\n",
            "when input is [25, 17], the target is 27\n",
            "when input is [25, 17, 27], the target is 10\n",
            "when input is [25, 17, 27, 10], the target is 0\n",
            "when input is [25, 17, 27, 10, 0], the target is 21\n",
            "when input is [25, 17, 27, 10, 0, 21], the target is 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1], the target is 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpliXu4AWnfc",
        "outputId": "aa4a1805-5d77-4b42-f907-f220149ac3d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "-jWpcHn0FK2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "Sa8hXq2X10AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "wUmjXHoh2rpf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class LayerNorm(nn.Module):\n",
        "#   def __init__(self, dim, eps=1e-5):\n",
        "#     self.eps = eps\n",
        "      # gamma and beta are trainable variables\n",
        "#     self.gamma = torch.ones(dim)\n",
        "#     self.beta = torch.ones(dim)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     xmean = x.mean(1, keepdim=True)\n",
        "#     xvar = x.var(1, keepdim=True)\n",
        "#     xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "#     out = self.gamma * xhat + self.beta\n",
        "#     return out\n",
        "\n",
        "#   def parameters(self):\n",
        "#     return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "e8zZsSkh10AH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### self-attention head with dropout"
      ],
      "metadata": {
        "id": "0rKspkVb3A8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    \"\"\"self-attention head.\n",
        "\n",
        "    there will be multiple heads. head_size * n_head = n_embed.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "    # tril is not the parameter of the module, so we call it a buffer\n",
        "    # (not a parameter)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, head_size)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * C ** -0.5# (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x) # (B, T, head_size)\n",
        "    out = wei @ v # (B, T, head_size)\n",
        "    return out"
      ],
      "metadata": {
        "id": "ovuFWAYe3Acc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention with a projection"
      ],
      "metadata": {
        "id": "AGzrj5Ms10AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([SelfAttentionHead(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embed, n_embed)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.proj(attention_out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "o-Etq2Uc10AI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FeedForward with dropout"
      ],
      "metadata": {
        "id": "vuVOS4Zt10AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embed):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embed, n_embed * 4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_embed * 4, n_embed),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "UZYDYjV610AI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block of (self-attention and feed-forward)"
      ],
      "metadata": {
        "id": "6_H-cvaw10AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size=head_size)\n",
        "    self.ffwd = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "8GCmWoNJ10AJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model with multiple layers of (self-attention, feed-forward) and layernorm + skipadd"
      ],
      "metadata": {
        "id": "jybCCY6J10AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "n_embed = 32\n",
        "n_head = 4\n",
        "n_layer = 3\n",
        "\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # an embedding is a table that maps each token to a vector\n",
        "    # nn.Embedding(n1, n2) maps a token whose value is up to n1 (starting from 0)\n",
        "    # to a vector of size n2. Here we map each token to a vector of size vocab_size\n",
        "    # (One example is one-hot encoding)\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embed) # final layer norm\n",
        "    # This is the output layer that maps the transformed embeddings back to the\n",
        "    # original vocab_size vector - logits.\n",
        "    self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"A Forward proporgation.\n",
        "\n",
        "    idx: the input. Dimension [B, T].\n",
        "    targets: target. Dimension [B, T], [B, i]th element is the target of sequence\n",
        "      idx[B, :i].\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # idx and targets are both (B, T), meaning (batch_size, block_size)\n",
        "    token_embed = self.token_embedding_table(idx) # (B, T, C = n_embed)\n",
        "    # pos_embed = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "    pos_embed = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
        "\n",
        "    x = token_embed + pos_embed\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x) # (B, T, C = vocab_size)\n",
        "    # here is a bit tricky, please refer to\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
        "    # to understand what's in there\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B * T, C)\n",
        "      targets = targets.view(B * T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"Generate the next token for max_new_tokens of times for the batch idx.\n",
        "\n",
        "    idx: a batch of samples, of size (B, T).\n",
        "    max_new_tokens: int, how many tokens to be generated given a sample.\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop the input size so that it's at most the same as block_size when\n",
        "      # generating\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # get the predictions\n",
        "      logits, loss = self(idx_cond) # logits: (B, T, C)\n",
        "      # print(logits.shape)\n",
        "      # Last one of the sequence\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
        "    return idx\n",
        "\n",
        "model = GPT()\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68792e7f-f495-4c37-9768-1618e6b93c08",
        "id": "QdxWeyaA10AJ"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.2523, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6CLZ5TD10AJ",
        "outputId": "32d8d56c-ee46-4e7f-8014-0e4d89ac5bc6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42369"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "hK9UEXSWAQtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jmWkQOi1A-un"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "LV-YUNBfPEB2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To tell Pytorch that everything inside this function will not be used in\n",
        "# loss.backward() so that it won't allocate any memory for the local variables\n",
        "# for backproporgation.\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "1j2eUN0RPmy0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for iter in range(max_iters):\n",
        "  if iter % eval_interval == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # sample a batch of data\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate the loss\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xatxcLsGBFUV",
        "outputId": "ca0acd94-9732-4ac8-98ef-4db364d4a5f7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3104, val loss 4.3100\n",
            "step 500: train loss 2.4657, val loss 2.4560\n",
            "step 1000: train loss 2.3117, val loss 2.3276\n",
            "step 1500: train loss 2.2432, val loss 2.2604\n",
            "step 2000: train loss 2.1860, val loss 2.2131\n",
            "step 2500: train loss 2.1573, val loss 2.1901\n",
            "step 3000: train loss 2.1427, val loss 2.1602\n",
            "step 3500: train loss 2.1033, val loss 2.1457\n",
            "step 4000: train loss 2.0895, val loss 2.1274\n",
            "step 4500: train loss 2.0572, val loss 2.0928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "j7HmiyGgBwZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)\n",
        "\n",
        "print(decode(model.generate(idx, max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jyE9E4tCC5o",
        "outputId": "a1bff5f3-65f2-4785-e6a2-b0c810b275ef"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MEONTUCES:\n",
            "Leash for hous?\n",
            "Taire tome and at andients for perecaint:\n",
            "Agnes notes'd me; ou me staed thoGood pettrane:\n",
            "I do meng\n",
            "Fich'd sownerreawirs that she aman: le golk a ne yough, gukit, mane mopinair!\n",
            "Arve; the loot.\n",
            "\n",
            "I wethyit? pare,\n",
            "The pliivery of not oun, cony the him wo af that's of mittees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the checkpoint"
      ],
      "metadata": {
        "id": "0CYBBTJUCDSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "s4RsNgJNCX08",
        "outputId": "1c7ce1b7-4684-49af-8543-e503eae6901f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./checkpoints\""
      ],
      "metadata": {
        "id": "hJurWs8UCaVi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMJz5dVpDENK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}